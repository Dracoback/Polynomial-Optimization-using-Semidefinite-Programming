% !TEX root = mythesis.tex


	\chapter{Polynomial Optimization}		% chapter 4
	\label{PolyOpchap}		% for reference (\eqref{introchap})
	
	 The focus of this chapter will be on polynomial optimization. We will start with nonnegative polynomials and discuss how some of them may be represented as a sum of squares, then we will show the connection between sum of squares polynomials and semidefinite matrices. The motivation behind this is that we wish to formulate our optimization problems as
	 $$
	 \textnormal{Maximize } \gamma \quad \textnormal{subject to } \quad p(x_1, \dots, x_n) - \gamma \quad \textnormal{is nonnegative},
	 $$
	 and our goal is to determine in what ways we may apply semidefinite programming to polynomial optimization. The bulk of this information will be adapted from Chapter 3 of \cite{BlekhermanGrigoriy;ParriloPabloA.;Thomas2013}.
	 
	 \section{Nonnegative Polynomials}
	 
	Our discussion will consider polynomials with $n$ variables with real coefficients. A multivariate polynomial $p(x_1, \dots, x_n)$ is \emph{nonnegative} if it takes only nonnegative values, i.e., 
	\begin{equation}
		p(x_1, \dots, x_n) \geq 0 \qquad \textnormal{for all } (x_1, \dots, x_n) \in \mathbf{R}^n.
	\end{equation}
	There are a number of natural questions about nonnegative polynomials to consider going forward:
	\begin{itemize}
		\item How do we decide if a polynomial $p(x)$ is nonnegative?
		
		\item Is it possible to certify whether or not a polynomial is nonnegative efficiently?
		
		\item How computationally complex is it to decide if a polynomial is nonnegative?
		
		\item Are there any features of the structure of the set of nonnegative polynomials that we can exploit?
	\end{itemize}
	Before we answer these questions, we will first take a look at some simple special cases.
	
	\subsection{Univariate Polynomials}
	
	We will start with the case of polynomials that only have a single variable, i.e., when $n = 1$:
	\begin{equation} \label{univariate polynomial}
		p(x) = p_d x^d + p_{d-1} x^{d-1} + \dots + p_1 x + p_0.
	\end{equation}
	We normally assume that the leading coefficient $p_d$ is nonzero, and it sometimes can be helpful to normalize the polynomial $p(x)$ to make it \emph{monic}, i.e., we make $p_d = 1$. We say that the \emph{roots} of $p(x)$ are the values of $x$ at which $p(x) = 0$. By the fundamental theorem of algebra, there is a unique factorization
	\begin{equation}
		p(x) = p_d * \prod\limits_{i=1}^{d} (x - x_i), 
	\end{equation}
	where the (complex) roots $x_i$ may have multiplicities, meaning that they are not all necessarily distinct.
	
	In this case, we can clearly tell that in order for $p(x)$ to be nonnegative, it is a necessary condition that the degree of $p(x)$ be even. If the degree is odd, then $p(x)$ will become negative as either $x \rightarrow \infty$ or $x \rightarrow -\infty$. 

	\subsection{Multivariate Polynomials}
	
	Let $P_{n, 2d}$ be the set of nonnegative polynomials that have $n$ variables of degree less than or equal to $2d$, i.e.,
	$$
		P_{n, 2d} = \{p \in \mathbf{R}[x]_{n, 2d} \mid p(x) \geq 0, \quad \forall x \in \mathbf{R}^n \}.
	$$
	If we identify a polynomial with its $N := \binom{n+d}{d}$ coefficients, and we notice that the constraints $p(x) \geq 0$ are affine in the coefficients of $p$ for every fixed $x$, it follows directly that $P_{n, 2d}$ is a \emph{convex} set in $\mathbf{R}[x]_{n, 2d} \sim \mathbf{R}^N$. We also have that the following is true.
	
	\begin{theorem}
		The set of nonnegative polynomials $P_{n, 2d}$ is a proper cone (closed, convex, pointed, and solid) in $\mathbf{R}[x]_{n, 2d} \sim \mathbf{R}^N$. 
	\end{theorem}
	
	\begin{proof}
		In order to show that $P_{n, 2d}$ is a proper cone, we need to show that it is closed, convex, solid, and pointed. We should also show that it is indeed a cone.
		
		$\mathbf{Cone: }$ $\forall \lambda \geq 0$, $p \in P_{n,2d}$, $\lambda p(x) \geq 0$. 
		
		$\mathbf{Closed: }$ Noticing that $p(x) \geq 0$ defines a closed half space for each fixed $x$, we can consider $P_{n, 2d}$ to be the infinite intersection of closed half spaces, which means that $P_{n, 2d}$ is closed. 
		
		$\mathbf{Convex: }$ $\forall p, q \in P_{n, 2d}$, $0 \leq \lambda \leq 1$, we have that $\forall x \in \mathbf{R}^n$, 
		$$
		\lambda p(x) + (1-\lambda) q(x) \geq 0
		$$
		since $0 \leq \lambda \leq 1$ and $p, q \in P_{n, 2d}$. Thus, $\lambda p(x) + (1- \lambda) q(x) \in P_{n, 2d}$, so $P_{n, 2d}$ is convex. 
		
		$\mathbf{Solid: }$ We want to show that we have a non-empty interior. The interior here would be $\textnormal{int}(P_{n, 2d}) = \{p \in \mathbf{R}[x]_{n, 2d} \mid p(x) > 0, \textnormal{ } \forall x \in \mathbf{R}^n\}.$ We see here by simple example that $p(x) = 1 + \sum_{i=1}^{n} x_{i}^{2d} \in \textnormal{int}(P_{n,2d})$. Therefore, $P_{n, 2d}$ is solid.
		
		$\mathbf{Pointed: }$ If $p \in P_{n, 2d}$ and $p \in -P_{n, 2d}$, then $p(x) \geq 0$ and $-p(x) \geq 0$, which implies that $p(x) = 0$. Thus, $P_{n, 2d}$ is pointed.
	\end{proof}
	
	\section{Sum of Squares}
	
	A multivariate polynomial $p(x_1, \dots, x_n)$ is a \emph{sum of squares} (sos) if it can be written as the sum of squares of some other polynomials. Formally, a polynomial $p(x) \in \mathbf{R}[x]_{n, 2d}$ is a sum of squares (sos) if there exist $q_1, \dots, q_m \in \mathbf{R}[x]_{n,d}$ such that 
	\begin{equation} \label{sos}
		p(x) = \sum_{k=1}^{m} q_k^2(x). 
	\end{equation}
	We will use $\sum_{n, 2d}$ to denote the set of sos polynomials with $n$ variables of degree less than or equal to $2d$. If a polynomial $p(x) \in \sum_{n, 2d}$, then it is obvious that $p(x) \geq 0$ for all $x \in \mathbf{R}^n$. This shows that a polynomial that is sos is immediately a nonnegative polynomial, i.e., $\sum_{n, 2d} \subseteq P_{n,2d}$. In general, sos decompositions are not unique. We can quickly see from this that $\sum_{n, 2d}$ is a convex cone. In fact, we will show later that $\sum_{n, 2d}$ is actually a proper cone.
	
	%\begin{theorem}
	%	The set of sos polynomials $\sum_{n, 2d}$ is a proper cone (closed, convex, pointed, and solid) in $\mathbf{R}[x]_{n, 2d} \sim \mathbf{R}^N$.
	%\end{theorem}
	
	%	\begin{proof}
	%		In order to show that $\sum_{n, 2d}$ is a proper cone, we need to show that it is closed, convex, solid, and pointed. We should also show that it is indeed a cone.
			
	%		$\mathbf{Cone: }$ $\forall \lambda \geq 0$, $p \in P_{n,2d}$, $\lambda p(x) = \lambda \sum_{k=1}^{m} q_k^2 (x) = \sum_{k=1}^{m} [\sqrt{\lambda} q_k (x)]^2 \in \sum_{n, 2d}$. Thus, $\sum_{n, 2d}$ is a cone. 
			
	%		$\mathbf{Closed: }$ \blue{ Stuck, could use some help.}
			
	%		$\mathbf{Convex: }$ $\forall p, r \in \sum_{n, 2d}$, $0 \leq \lambda \leq 1$, we have that $\forall x \in \mathbf{R}^n$, 
	%		$$
	%		\lambda p(x) + (1-\lambda) r(x) = \sum_{k=1}^{m_1} [\sqrt{\lambda} q_k (x)]^2 + \sum_{j=1}^{m_2} [\sqrt{1 - \lambda} q_j (x)]^2 \in \sum_{n, 2d}
	%		$$
	%		since $0 \leq \lambda \leq 1$ and $p, r \in \sum_{n, 2d}$. Thus, $\lambda p(x) + (1- \lambda) r(x) \in \sum_{n, 2d}$, so $\sum_{n, 2d}$ is convex. 
			
	%		$\mathbf{Solid: }$ We want to show that we have a non-empty interior. We see here by simple example that $p(x) = 1 + x^{2d} \in \textnormal{int}(\sum_{n, 2d})$. Therefore, $\sum_{n, 2d}$ is solid. \blue{ Something feels off about this. I am not sure I have a good grasp on what the interior of this set would be.}
			
	%		$\mathbf{Pointed: }$ If $p \in \sum_{n, 2d}$ and $p \in -\sum_{n, 2d}$, then $p(x)$ is nonnegative since $p \in \sum_{n, 2d}$ and $p(x)$ is nonpositive since $p \in -\sum_{n, 2d}$, which implies that $p(x) = 0$. Thus, $\sum_{n, 2d}$ is pointed.
	%	\end{proof}
	
	\subsection{When is nonnegativity equal to sum of squares?}
	
	Now that we know that $p(x) \in \sum_{n, 2d} \implies p(x) \in P_{n,2d}$, it is natural to ask when the converse holds true, i.e., when a nonnegative polynomial can be expressed as a sum of squares. David Hilbert proved that $P_{n,2d} = \sum_{n,2d}$ only in the following three cases:
	\begin{itemize}
		\item Univariate polynomials (i.e., $n = 1$).
		
		\item Quadratic polynomials ($2d = 2$).
		
		\item Bivariate quartics ($n = 2, 2d = 4$). 
	\end{itemize}
	For all other cases, there always exist nonnegative polynomials that are \emph{not} sums of squares. We will prove the first two cases. 
	
	\begin{proof} [Univariate nonnegative polynomials are sos]
		Suppose we have a univariate nonnegative polynomial $p(x) = p_{2d} x^{2d} + \dots + p_{1} x + p_{0}$. Utilizing the fundamental theorem of algebra, this polynomial has a factorization of the form
		$$
		p(x) = p_{2d} * \prod_{j} (x - r_j)^{n_j} * \prod_{k} [(x - z_k)(x-z_k^*)]^{m_k},
		$$
		where $r_j$ and $(z_k, z_k^*)$ are the real and complex roots of $p(x)$. Because $p(x)$ is a univariate nonnegative polynomial, we know that $p_{2d} > 0$ because of the necessary end behavior in order to guarantee nonnegativity. For our complex roots, we can write $z_k = a_k + ib_k$ and $z_k^* = a_k - ib_k$. This allows us to rewrite part of the above formulation of $p(x)$ as 
		$$
		(x - z_k)(x -z_k^*) = (x - a_k - ib_k)(x - a_k + ib_k) = x^2 - 2a_k x + a_k^2 + b_k^2 = (x - a_k)^2 + b_k^2.
		$$
		Thus, that portion of the polynomial is a sum of squares, and as such is positive. 
		
		Next we will show that the $n_j$'s must be even. Assume that there exists an $n_j$ which is odd. If we focus on this particular exponent, which we will call $n_{j_{\textnormal{odd}}}$, then the root $r_{j_{\textnormal{odd}}}$ must cause our polynomial to change signs, which contradicts our nonnegativity condition for $p(x)$. Thus, each of the $n_j$'s must be even.
		
		Finally, we will show that our polynomial is a sum of squares. Suppose that there exist two polynomials $q(x)$ and $t(x)$ which are sums of squares. Thus, they can be written in the forms $q(x) = \sum_{c=1}^{u} f_{c}^2 (x)$ and $t(x) = \sum_{d=1}^{v} g_{d}^2 (x)$. If we take their product, we obtain 
		
		$$\begin{aligned}
		q(x)t(x) = \left(\sum_{c=1}^{u} f_{c}^2 (x)\right)\left(\sum_{d=1}^{v} g_{d}^2 (x)\right) &= \sum_{c=1}^{u} \left[f_{c}^2 (x)\left(\sum_{d=1}^{v} g_{d}^2 (x)\right)\right]\\
		&= \sum_{c=1}^{u} \left[\sum_{d=1}^{v} f_{c}^2 (x)g_{d}^2 (x)\right]\\
		&= \sum_{c=1}^{u} \left[\sum_{d=1}^{v} \left(f_{c}(x) *g_{d} (x)\right)^2\right].
		\end{aligned}$$
		As we can see, this means that a product of a sum of squares will also be a sum of squares. Since we are multiplying squares and sums of squares in order to form $p(x)$, it is indeed a sum of squares. 
	\end{proof}
	
	\begin{proof} [Quadratic nonnegative polynomials are sos]
		Let $p(x)$ be a quadratic polynomial in $n$ variables. According to page 51 of \cite{BlekhermanGrigoriy;ParriloPabloA.;Thomas2013}, every quadratic polynomial can be represented as 
		$$
		p(x) = x^T A x + 2b^T x + c, 
		$$
		where $A \in \mathcal{S}^n$ is a symmetric matrix. We can factor this as
		$$
		p(x) = x^T A x + 2b^T x + c = \begin{bmatrix}
		x \\ 1
		\end{bmatrix}^T \begin{bmatrix} A & b \\ b^T & c \end{bmatrix} \begin{bmatrix}
		x \\ 1
		\end{bmatrix} = v^T Q v.
		$$
		Also according to page 51 of \cite{BlekhermanGrigoriy;ParriloPabloA.;Thomas2013}, since $p(x)$ is nonnegative, we are guaranteed that 
		$$
		\begin{bmatrix} A & b \\ b^T & c \end{bmatrix} = Q \succeq 0.
		$$
		This implies that we can rewrite $Q = M^TM$, and as such 
		$$
		p(x) = \begin{bmatrix}
		x \\ 1
		\end{bmatrix}^T M^T M \begin{bmatrix}
		x \\ 1
		\end{bmatrix}
		=
		||M \begin{bmatrix}
		x \\ 1
		\end{bmatrix}||_2^2,
		$$
		which shows that $p(x)$ is indeed a sos polynomial.
	\end{proof}
	
	\section{Sum of Squares and Semidefinite Matrices}
	
	At this point, a natural question is how one would decide if a polynomial $p(x)$ belongs to the cone $\sum_{n, 2d}$. As we will explore, this is related to its connection to semidefinite programming. 
	
	\subsection{Univariate Polynomials}
	
	We will begin with the univariate case. Consider a univariate polynomial $p(x)$ of degree $2d$:
	\begin{equation}
		p(x) = p_{2d}x^{2d} + p_{2d-1}x^{2d-1} + \dots + p_1 x + p_0.
	\end{equation}
	Assume that $p(x)$ is a sum of squares, i.e., it can be expressed as in \eqref{sos}. It is important to note that the degree of the polynomials $q_k$ used for the sum of squares representation can be of degree of at most $d$. We can then write
	\begin{equation} \label{sos matrix decomp}
		\begin{bmatrix}
		q_1 (x) \\ q_2 (x) \\ \vdots \\ q_m (x)
		\end{bmatrix}
		=
		V \begin{bmatrix}
		1 \\ x \\ \vdots \\ x^d
		\end{bmatrix},
	\end{equation}
	where $V \in \mathbf{R}^{m \times (d+1)}$, and the $k$th row of $V$ contains the coefficients for the polynomial $q_k$. Define $[x]_d$ as the vector of monomials on the right-hand side of \eqref{sos matrix decomp}, and define the matrix $Q := V^T V$. Then, we have
	$$
	p(x) = \sum_{k=1}^{m} q_k^2 (x) = (V[x]_d)^T (V[x]_d) = [x]_d^T V^T V [x]_d = [x]_d^T Q [x]_d.
	$$
	This result leads to the following lemma.
	\begin{lemma}
		Let $p(x)$ be a univariate polynomial of degree $2d$. Then $p(x)$ is a sum of squares if and only if there exists a symmetric matrix $Q \in \mathcal{S}^{d+1}$ that satisfies
		\begin{equation} \label{sos matrix condition}
		p(x) = [x]_d^T Q [x]_d, \qquad Q \succeq 0.
		\end{equation}
	\end{lemma}
	
	We typically call the matrix $Q$ the \emph{Gram matrix} of the sum of squares representation. One part of this lemma follows from our properties of semidefinite matrices, as constructing the matrix $Q$ as $V^T V$ immediately gives us that $Q$ is positive semidefinite. For the other direction, assume that there exists a positive semidefinite matrix $Q$ which satisfies \eqref{sos matrix condition}. From there, we can factorize $Q = V^T V$ by using either Cholesky or square root factorization. This will provide us with our sum of squares decomposition of $p(x)$. 
	
	The condition \eqref{sos matrix condition} provides us with a semidefinite program. To see this, notice that the constraint $p(x) = [x]_d^T Q [x]_d$ is \emph{affine} in the matrix $Q$, and as such the set of possible Gram matrices $Q$ is given by the intersection of an affine subspace and the cone of positive semidefinite matrices. To obtain explicit equations for this semidefinite program, we will index the rows and columns of $Q$ by $\{0, \dots, d\}$ as 
	$$
	[x]_d^T Q [x]_d = \sum_{i=0}^{d} \sum_{j=0}^{d} Q_{ij} x^{i+j}= \sum_{k = 0}^{2d} \left( \sum_{i+j = k}^{} Q_{ij} \right) x^k.
	$$
	So for this expression to be equal to $p(x)$, we must have that 
	\begin{equation} \label{coefficient matrix equality}
		p_k = \sum_{i + j = k}^{} Q_{ij}, \qquad k = 0, \dots, 2d.
	\end{equation}
	This provides us with a system of $2d+1$ linear equations between the entries of $Q$ and the coefficients of $p(x)$. Therefore, since $Q$ is constrained to be positive semidefinite while also belonging to the affine subspace that is defined by our system of linear equations, we have that our sos condition is equivalent to a semidefinite programming problem. This means that we have shown the following lemma. 
	\begin{lemma} \label{univariate polynomial sos condition}
		A univariate polynomial $p(x) = \sum_{k=0}^{2d} p_k x^k$ is a sum of squares if and only if there exists a positive semidefinite matrix $Q \in \mathcal{S}^{d+1}$ satisfying \eqref{coefficient matrix equality}.
	\end{lemma}
	Note that we have previously shown that nonnegativity and sum of squares are equivalent conditions in the univariate case. Thus, Lemma \ref{univariate polynomial sos condition} completely characterizes the set of univariate nonnegative polynomials and shows that the sets $P_{1, 2d}$ and $\sum_{1, 2d}$ are equal. 
	
	\subsection{Multivariate Polynomials}
	
	We will now discuss the multivariate case. Consider a polynomial $p(x_1, x_2, \dots, x_n)$ of degree $2d$ in $n$ variables. The number of coefficients for $p$ is $\binom{n+2d}{2d}$. We will let $p(x) = \sum_{\alpha} p_{\alpha} x^{\alpha}$, where $\alpha$ are tuples of exponents $\alpha \in \{ (\alpha_1 , \alpha_2, \dots, \alpha_n) \mid \alpha_1 + \dots + \alpha_n \leq 2d, \alpha_i \geq 0 \quad \forall i = 1, \dots, n \}.$
	
	Let $[x]_d := [1, x_1, \dots, x_n, x_1^2, x_1 x_2, \dots, x_n^d]^T$ be the vector of all $\binom{n+d}{d}$ monomials in $x_1, \dots, x_n$ of degree less than or equal to $d$, and consider the equation 
	\begin{equation} \label{multivariate poly sos}
		p(x) = [x]_d^T Q [x]_d,
	\end{equation}
	where $Q$ is an $\binom{n+d}{d} \times \binom{n+d}{d}$ symmetric matrix. Following the process used to arrive at \eqref{coefficient matrix equality} - that is, indexing the matrix $Q$ by the $\binom{n+d}{d}$ monomials in $n$ variables with their associated exponent tuples - allows us to obtain the following conditions:
	\begin{equation} \label{coefficient matrix equality multivariate}
		p_{\alpha} = \sum_{\beta + \gamma = \alpha} Q_{\beta \gamma}, \qquad Q \succeq 0.
	\end{equation}
	This provides us with a system of $\binom{n+2d}{2d}$ linear equations, one for each coefficient of $p(x)$. As before, these equations are affine conditions that relate the entries of $Q$ to the coefficients of $p(x)$. This allows us to decide if a polynomial is a member of, or optimize over, the set of sos polynomials by solving an SDP program. This relationship between positive semidefinite matrices and sum of squares is summarized by the following theorem:
	\begin{theorem} \label{multivariate sos summary theorem}
		A multivariate polynomial $p(x) = \sum_{\alpha} p_{\alpha} x^{\alpha}$ in $n$ variables and degree $2d$  is a sum of squares if and only if there exists $Q \in \mathcal{S}_{+}^{\binom{n+d}{d}}$ that satisfies \eqref{coefficient matrix equality multivariate}. As a consequence, membership in $\sum_{n, 2d}$ can be decided via semidefinite programming. 
	\end{theorem}
	
	The size of the matrix of the semidefinite program that appears in \eqref{multivariate sos summary theorem} is $\binom{n+d}{d}$, which grows polynomially in the number of variables $n$ for a fixed degree $d$. If $n$ is fixed, it also grows polynomially in $d$ since $\binom{n+d}{d} = \binom{n+d}{n}$. 
	
	Now that we have shown the connection between semidefinite matrices and sos polynomials, we can more easily show that the set of sos polynomials forms a proper cone.
	
	\begin{theorem}
		The set of sos polynomials $\sum_{n, 2d}$ is a proper cone (closed, convex, pointed, and solid) in $\mathbf{R}[x]_{n, 2d} \sim \mathbf{R}^N$.
	\end{theorem}
	
	\begin{proof}
		Consider a polynomial $p(x) \in \sum_{n, 2d}$. We have shown previously that there exists a representation of $p(x)$ as $[x]_d^T Q [x]_d$, where $Q \succeq 0$. Because we can represent all possible $p(x)$ as a positive semidefinite matrix, and we have shown that $\mathcal{S}_+^n$ is a proper cone, it follows that $\sum_{n, 2d}$ is also a proper cone. We will elaborate further:
		
		$\mathbf{Convex: }$ Let $p(x) = \sum_{i=1}^{a} q_i^2 (x) \in \sum_{n, 2d}$ and $r(x) = \sum_{j=1}^{b} s_j^2 (x) \in \sum_{n, 2d}$. Then for $0 \leq \lambda \leq 1$, we have 
		$$
		\lambda p(x) + (1 - \lambda) r(x) = \lambda \sum_{i=1}^{a} q_i^2 (x) + (1 - \lambda) \sum_{j=1}^{b} s_j^2 (x) = \sum_{i=1}^{a} (\sqrt{\lambda}q_i(x))^2 + \sum_{j=1}^{b} (\sqrt{1 - \lambda}s_j(x))^2 \in \sum_{n, 2d}.
		$$
		Thus, $\sum_{n, 2d}$ is convex.
		
		$\mathbf{Pointed: }$ If $p \in \sum_{n, 2d}$ and $p \in -\sum_{n, 2d}$, then $p(x) \geq 0$ and $-p(x) \geq 0$, which implies that $p(x) = 0$. Thus, $\sum_{n, 2d}$ is pointed.
		
		$\mathbf{Solid: }$ Choose $Q$ to be the identity matrix. This gives us a representation for a sos polynomial, and because of our properties of $\mathcal{S}_n^+$ we know that it is in the interior of the set of sos polynomials. Thus, $\sum_{n, 2d}$ is solid. 
		
		$\mathbf{Closed: }$  Noticing that $p(x) \geq 0$ defines a closed half space for each fixed $x$, we can consider $\sum_{n, 2d}$ to be the infinite intersection of closed half spaces, which means that $\sum_{n, 2d}$ is closed. 
	\end{proof}
	
	\section{Polynomial Optimization with Sum of Squares Programming}
	
	In this section, we will discuss how we can use sum of squares and semidefinite programming in order to solve both unconstrained and constrained optimization problems for univariate and multivariate polynomials. 
	
	\subsection{Unconstrained Univariate Polynomial Optimization}
	
	Our first application will be our simplest, as we will demonstrate how to perform global optimization of a univariate polynomial $p(x)$. While there are many other methods that could handle this task, walking through it will help us outline certain features for our more complicated applications. 
	
	Instead of computing a minimizer $x_*$ which results in a global minimum $p(x_*)$, we will instead frame our approach as obtaining a best possible lower bound on the optimal value of $p(x)$. Clearly a number $\gamma$ is a global lower bound for $p(x)$ if and only if the polynomial $p(x) - \gamma$ is nonnegative. In other words, for all $x \in \mathbf{R}$,  
	$$
	p(x) \geq \gamma \iff p(x) - \gamma \geq 0.
	$$
	This naturally leads to the following optimization problem:
	\begin{equation} \label{Opt-Nn} \tag{OPT-NN}
		\textnormal{Maximize } \gamma \quad \textnormal{subject to } \quad p(x) - \gamma \quad \textnormal{is nonnegative}.
	\end{equation}
	Because this problem is defined by an infinite number of linear inequalities (one for each value of $x$), this is a convex optimization problem. Its optimal solution $\gamma_*$ is equal to the global minimum of the polynomial $p(x_*)$. Now let us consider the following optimization problem: 
	\begin{equation} \label{Opt-Sos} \tag{OPT-SOS}
	\textnormal{Maximize } \gamma \quad \textnormal{subject to } \quad p(x) - \gamma \quad \textnormal{is sos}.
	\end{equation}
	The only difference between the two above formulas comes from the replacement of the nonnegativity condition with a sum of squares condition. As we have shown previously, these two conditions are equivalent for the univariate case. The form \eqref{Opt-Sos} is in the form of a sos program, which means that it is equivalent to a semidefinite program for us to solve. This results in us being able to use semidefinite programming to find the global minimum value for $p(x)$. Notice that even if $p(x)$ is highly nonconvex, we can still find its global minimum with convex optimization. 
	
	\subsection{Multivariate Unconstrained Polynomial Optimization}
	
	We will now consider the multivariate case of unconstrained polynomial optimization. Following our framework for the formulation of the univariate case, we can formulate the global minimum of a multivariate polynomial $p(x_1, \dots, x_n)$ as follows:
	
	\begin{equation} \label{Mopt-Nn} \tag{MOPT-NN}
	\textnormal{Maximize } \gamma \quad \textnormal{subject to } \quad p(x_1, \dots, x_n) - \gamma \quad \textnormal{is nonnegative}.
	\end{equation}
	
	Since we do not wish for our constraint set to be nonnegative polynomials and would instead prefer to work with sos constraints, we can reformulate the global minimum as:
	
	\begin{equation} \label{Mopt-Sos} \tag{MOPT-S0S}
	\textnormal{Maximize } \gamma \quad \textnormal{subject to } \quad p(x_1, \dots, x_n) - \gamma \quad \textnormal{is sos}.
	\end{equation}
	
	Note that this time, we are not guaranteed an exact solution by solving \eqref{Mopt-Sos} because we cannot guarantee equality between nonnegativity and sos in the multivariate case. However, we can use it to establish a bound on our desired value. Let $p_*$ be the optimal value of \eqref{Mopt-Nn} and $p_{sos}$ be the optimal value of \eqref{Mopt-Sos}. Because we are not guaranteed equality, we have the inequality 
	$$
	p_{sos} \leq p_*,
	$$
	providing us with a lower bound on the global minimum of $p(x_1, \dots, x_n)$. This is expected since multivariate polynomial optimization is NP-hard while semidefinite programming is polynomial-time to a given accuracy. 
	
	\subsection{Constrained Optimization}

	It is natural to question how we may rewrite our optimization problems if we only are concerned with a polynomial $p(x)$ being nonnegative over a subset $S \subset \mathbf{R}^n$. In other words, is there a way to consider constraints in our optimization formulation? We will consider separately sets $S$ defined by equalities, inequalities, and a combination of them.
	
	We will start by considering equality constraints. Consider a set $S$ described by polynomial equations, i.e. of the form
	$$
	S = \{x \in \mathbf{R}^n \mid f_1(x) = 0, \dots, f_m(x) = 0\}.
	$$
	Drawing parallels to weak duality and Lagrange multipliers, we will write our constraints as the following condition:
	\begin{equation} \label{equal constraints}
		p(x) + \sum_{i=1}^{m} \lambda_i(x) f_i(x) \quad \textnormal{is sos},
	\end{equation}
	where $\lambda_i(x)$ are arbitrary polynomials. This condition obviously implies that $p(x)$ is nonnegative on $S$, since our summation disappears and we are left with a sos (and thus, nonnegative) $p(x)$. If we restrict the degrees of the $\lambda_i$'s, our condition has the form of an sos program.
	
	Now let us consider sets $S$ that are formed using inequalities. Consider a set $S$ described by polynomial inequalities, i.e. of the form
	$$
	S = \{x \in \mathbf{R}^n \mid g_1(x) \geq 0, \dots, g_m(x) \geq 0\}.
	$$
	Following a similar approach that resulted \eqref{equal constraints}, we can consider expressing $p(x)$ as 
	\begin{equation} \label{inequal constraints}
		p(x) = s_0 (x) + \sum_{i=1}^{m} s_i (x) g_i (x),
	\end{equation}
	where $s_0 (x)$ and $s_i (x)$ are sos polynomials. By doing this, we guarantee that $p(x)$ will be nonnegative on the set $S$ since all parts of the representation will be positive. This would give a way of expressing the constraining set as a part of our optimization problem.
	
	It is natural to ask how we may certify nonnegativity on a set $S$ defined by both polynomial equations and inequalities, i.e. of the form 
	$$
	S = \{x \in \mathbf{R}^n \mid f_1 (x) = 0, \dots, f_k (x) = 0, g_1 (x) \geq 0, \dots, g_m (x) \geq 0\}.
	$$
	Simply put, we would combine \eqref{equal constraints} and \eqref{inequal constraints} in order to have conditions of the form
	\begin{equation}
		p(x) + \sum_{i=1}^{m} \lambda_i(x) f_i(x) \quad \textnormal{is sos},
	\end{equation}
	where $\lambda_i (x)$ are arbitrary polynomials and 
	\begin{equation}
		p(x) = s_0 (x) + \sum_{j=1}^{m} s_j (x) g_j (x),
	\end{equation}
	where $s_0 (x)$ and $s_j (x)$ are sos polynomials. These conditions would certify that $p(x)$ was nonnegative on the set $S$.
	
	\section{Numerical Results and Applications}
	
	Now that we have a framework for using sos polynomials and semidefinite programming in order to solve polynomial optimization problems, we can set up numerical experiments and application problems in order to test our two solvers, SCS Solver (which is using ADMM) and Mosek (which is using an interior-point method).
	
		\subsection{Numerical Experiments}
		
		In order to get a sense of how these two different methods of solving compare, we can run each of them through a set of benchmark problems. This will help us determine when one solver may be at an advantage over the other. Our first benchmark problem comes from \cite{BlekhermanGrigoriy;ParriloPabloA.;Thomas2013}, with the rest being randomly created custom problems. The results of our benchmarking are listed in the table below. For ease of reading, the CPU times are rounded to the nearest hundredth of a second. To see the code that we implemented for this table, visit $Github link here$.
		
		\begin{center}
			\begin{tabular}{||c | c | c | c | c | c||} 
				\hline
				Problem & n & m & 2d & SCS CPU & Mosek CPU \\
				\hline\hline
				Blekherman Exer. 3.62 & 1 & 1 & 4 & 0.00 & 0.00 \\
				\hline
				Custom Problem 01 & 2 & 1 & 4 & 0.03 & 0.00 \\
				\hline
				Custom Problem 02 & 3 & 1 & 4 & 0.29 & 0.03 \\
				\hline
				Custom Problem 03 & 4 & 1 & 4 & 0.16 & 0.02 \\
				\hline
				Custom Problem 04 & 4 & 2 & 4 & 0.13 & 0.02 \\
				\hline
				Custom Problem 05 & 4 & 3 & 4 & 0.40 & 0.02 \\
				\hline
				Custom Problem 06 & 4 & 4 & 4 & 0.23 & 0.02 \\
				\hline
				Custom Problem 07 & 4 & 4 & 6 & 1.60 & 0.11 \\
				\hline			
				Custom Problem 08 & 5 & 4 & 6 & 0.66 & 0.86 \\
				\hline
				Custom Problem 09 & 6 & 4 & 6 & 6.97 & 5.47 \\
				\hline
				Custom Problem 10 & 5 & 5 & 6 & 0.62 & 0.78 \\
				\hline
				Custom Problem 11 & 6 & 2 & 6 & 7.99 & 7.48 \\
				\hline
				Custom Problem 12 & 5 & 4 & 8 & 3.29 & 39.80 \\
				\hline
				Custom Problem 13 & 3 & 1 & 6 & 0.39 & 0.03 \\
				\hline
				Custom Problem 14 & 3 & 1 & 8 & 0.66 & 0.13 \\
				\hline
				Custom Problem 15 & 4 & 4 & 8 & 2.98 & 2.56 \\
				\hline
				Custom Problem 16 & 4 & 4 & 10 & 3.79 & 38.36 \\
				\hline
				Custom Problem 17 & 5 & 2 & 8 & 0.25 & 41.83 \\
				\hline
				Custom Problem 18 & 3 & 2 & 10 & 1.95 & 0.81 \\
				\hline
				Custom Problem 19 & 3 & 3 & 14 & 2.82 & 26.55 \\
				\hline				
			\end{tabular}
		\end{center} 
	The table points out some interesting trends. The number of variables and the number of constraints do not seem to have a profound impact on the time of computation on their own. The degree of the polynomial seems to have the greatest impact on the CPU time, especially for Mosek. 
	
	\subsection{Application: Wireless Coverage using Minimum Transmission}
	
	The work here recreates one of the application problems posed by \cite{Ahmadi2016a}.
	
	In this problem, we are given $n$ wireless electromagnetic transmitters whose positions $(\bar{x}_i , \bar{y}_i), \i = 1, \dots, n$ are known, and we are told that we would like to achieve a certain coverage level over certain areas around these transmitters while minimizing the overall transmission rates coming from the transmitters. Each transmitter emits waves in all directions with equal intensity. We are provided with an equation for determining the amount of energy $E_i$coming from each transmitter from the laws of electromagnetics:
	$$
	E_i (x,y) = \frac{c_i \lambda}{(x - \bar{x}_i)^2 + (y - \bar{y}_i)^2},
	$$
	where $c_i$ is the transmission rate from the $i$th device and $\lambda$ is some propagation constant, which going forward we will set to $1$ without loss of generalization. Our goal is to make sure that certain regions $\beta_j$ are guaranteed to receive a total energy of at least $C$ units, while minimizing the total transmission power of the transmitters. In addition, each transmitter has an upper limit on its transmission rate $\gamma_i$.
	
	To summarize, we are given the following values as input: $C$ (required coverage level), $\gamma_i$ (upper bounds on transmission rates), $(\bar{x}_i , \bar{y}_i)$ (location of our transmitters), and $\beta_j$, $j = 1, \dots, m$ (basic semialgebraic sets describing regions to be covered). Our goal is to find transmission rates $c_i$ to solve the following optimization problem:
	\begin{equation}
		\begin{aligned}
		\textnormal{Minimize } \qquad \qquad \sum_{i=1}^{n} c_i \\
		\textnormal{subject to } \qquad \qquad c_i \leq \gamma_i, & \quad \forall i = 1 \dots n,\\
		E(x,y) := \sum_{i=1}^{n} \frac{c_i}{(x - \bar{x}_i)^2 + (y - \bar{y}_i)^2}  \geq C, & \quad \forall (x,y) \in \beta_j, j = 1, \dots m.
		\end{aligned}
	\end{equation}	
	We can manipulate the last line of constraints in order to obtain a polynomial that must be nonnegative on our $\beta_j$'s. Then $\forall (x,y) \in \beta_j, j = 1, \dots, m$, we have
	\begin{equation} \label{p}
		p(x, y) := -C \Pi_{i = 1}^{n} [(x - \bar{x}_i)^2 + (y - \bar{y}_i)^2] + \sum_{i = 1}^{n} c_i \Pi_{k \neq i} [(x - \bar{x}_k)^2 + (y - \bar{y}_k)^2] \geq 0.
	\end{equation}
	The degree of this polynomial is two times the number of transmitters. We will let each set $\beta_j$ be defined as
	$$
		\beta_j = \{x : g_{j, 1}(x,y) \geq 0, \dots, g_{j, k_{j}}(x,y) \geq 0 \}, 
	$$
	for some bivariate polynomials $g_{j, 1}, \dots, g_{j, k_{j}}$. Basing the reformulation of our problem on \eqref{inequal constraints}, we end up with the following optimization problem:
	\begin{equation}
		\begin{aligned}
		\textnormal{Minimize } \qquad &\sum_{i=1}^{n} c_i \\
		\textnormal{subject to } p =& \sigma_{j, 0} + \sum_{i=1}^{k_{j}} \sigma_{j, i} g_{j, i}, & \quad j = 1 \dots m,\\
		&\sigma{j, 0}, \sigma{j, i}, & \quad \textnormal{sos},
		\end{aligned}
	\end{equation}
	where $p$ is the same as in \eqref{p} and $\sigma{j, 0}, \sigma{j, i}$ are bivariate polynomials whose degree is upper bounded by some even integer $d$. This provides us with a semidefinite program to solve.
	
	Our code for the following problems is available here: $\textnormal{insert github link here}$.
	
	\textbf{Example 1 - Two Transmitters and Five Regions}
	
	We now use this to solve a concrete example. We are given two transmitters whose upper bounds for their transmission rates are both $11$. The first transmitter is located at $(1, 1.5)$, while the second transmitter is located at $(2, 1)$. We are given five ellipsoidal regions to cover:
	$$
	\begin{aligned}
	&\beta_1 = \{(x,y) : 0.01-3(x-1.1)^2-2(x-1.1)*(y-1.75)-(y-1.75)^2 \geq 0\},\\
	&\beta_2 = \{(x,y) : 0.01-(x-1.25)^2-3(y-2)^2 \geq 0\},\\
	&\beta_3 = \{(x,y) : 0.01-(x-1.5)^2-(y-1.75)^2 \geq 0\},\\
	&\beta_4 = \{(x,y) : 0.01-(x-1.8)^2+2*(x-1.8)*(y-1.8)-3*(y-1.8)^2 \geq 0\},\\
	&\beta_5 = \{(x,y) : 0.02-5(x-2)^2-(y-1.4)^2 \geq 0\}.
	\end{aligned}
	$$
	Each of these areas has a required energy level of $10$. 
	
	A natural question to consider is if both transmitters are even necessary. If we remove one transmitter at a time from our problem, we see that transmitter 1 (located at $(1, 1.5)$) would need a transmission rate of $11.446$ and transmitter 2 (located at $(2,1)$) would need a transmission rate of $17.594$. As both of these numbers is above our upper bounds for transmission rates, we can clearly see that both transmitters will be necessary. When we include both transmitters and set our sos polynomials to degree $2$, we end up with an optimal solution of $c_1 = 2.554$ and $c_2  = 5.564$, for a cumulative transmission rate of $8.118$. 
	
	When we ran this example with both of our solvers, Mosek had no problems finding our optimal solution very quickly, with a CPU time of 0.02 seconds. SCS Solver, on the other hand, was unable to obtain the optimal solution. Even after increasing the number of iterations allowed, SCS Solver was unable to determine the optimal solution.  
	
	\textbf{Example 2 - Three Transmitters and Eight Regions}
	
	Now our company has acquired a third transmitter in this area, located at $(1.5, 2.5)$. In addition, we are given three more ellipsoidal regions to cover:
	$$
	\begin{aligned}
	&\beta_6 = \{(x,y) : 0.02-4(x-1.6)^2+2*(x-1.6)*(y-2.3)-2*(y-2.3)^2 \geq 0\},\\
	&\beta_7 = \{(x,y) : 0.01-3(x-0.9)^2-(y-2.3)^2 \geq 0\},\\
	&\beta_8 = \{(x,y) :0.02-2(x-1.55)^2+2*(x-1.55)*(y-2.8)-2*(y-2.8)^2 \geq 0\}.
	\end{aligned}
	$$
	Each of these areas also has a required energy level of $10$. 
	
	Running our solver on the updated problem, we see that it is indeed possible to obtain our goal of obtaining the required coverage over all of the regions. In fact, with our new transmitter, we end up having a smaller overall transmission rate! Our new solution was $c_1 = 0.305$, $c_2 = 1.830$, and $c_3 = 4.451$, resulting in a cumulative transmission rate of $6.586$.
	
	In fact, the numbers are so low that it is natural to ask if we even need all three towers. Can our newest transmission tower handle all of the regions on its own? In short, no. Without the other towers, we would need $c_3 = 17.969$. It then begs the question of if we need all three towers. Running some experiments, we determine that we can remove either of the first two transmitters and still be okay. However, it is impossible to cover all eight regions if we remove the newest transmitter.
	
	When we ran this example with both of our solvers, Mosek once again had no problems finding our optimal solution very quickly, with a CPU time of 0.06 seconds. SCS Solver, on the other hand, obtained an incorrect solution where each of the $c_i$'s was roughly $0.001$. 